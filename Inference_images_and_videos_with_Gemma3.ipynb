{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahreemrasul/gemma3_inference/blob/main/Inference_images_and_videos_with_Gemma3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHJthTgwCPZX"
      },
      "source": [
        "# Gemma 3: multimodal & multilingual capabilities\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/tahreemrasul/gemma3_inference/blob/main/Inference_images_and_videos_with_Gemma3.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/tahreemrasul/gemma3_inference/blob/main/Inference_images_and_videos_with_Gemma3.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je5cAuDze_yq"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) |  [Tahreem Rasul](https://github.com/tahreemrasul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVVLJKdOKxId"
      },
      "source": [
        "### Get access to Gemma 3\n",
        "\n",
        "Before using Gemma 3 for the first time, you must request access to the model through Hugging Face by completing the following steps:\n",
        "\n",
        "1. Log in to [Hugging Face](https://huggingface.co), or create a new Hugging Face account if you don't already have one.\n",
        "2. Go to the [Gemma 3 model card](https://huggingface.co/google/gemma-3-4b-it) to get access to the model.\n",
        "3. Complete the consent form and accept the terms and conditions.\n",
        "\n",
        "To generate a Hugging Face token, open your [**Settings** page in Hugging Face](https://huggingface.co/settings), choose **Access Tokens** option in the left pane and click **New token**. In the next window that appears, give a name to your token and choose the type as **Write** to get the write access.\n",
        "\n",
        "Then, in Colab, select **Secrets** (üîë) in the left pane and add your Hugging Face token. Store your Hugging Face token under the name `HF_TOKEN`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s74YuKdKbwF"
      },
      "source": [
        "### Select the runtime\n",
        "\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to load the Gemma 3 model. In this case, a T4/L4 GPU would be needed to load the model weights.\n",
        "\n",
        "1. In the upper-right of the Colab window, click the **‚ñæ (Additional connection options)** dropdown menu.\n",
        "1. Select **Change runtime type**.\n",
        "1. Under **Hardware accelerator**, select **T4 or L4**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr88Pc4wojbR"
      },
      "source": [
        "## Step 1: Install Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl62-Wq2Yhgy"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wx6ZJySomMZ"
      },
      "source": [
        "## Step 2: Import libraries and dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9u9RU4ZopHI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from IPython.display import Markdown, HTML\n",
        "from base64 import b64encode\n",
        "import requests\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zx5GotYouHh"
      },
      "source": [
        "## Step 3: Choose the Gemma 3 model variant.\n",
        "\n",
        "Gemma 3 is available in four sizes, each supporting different features:\n",
        "\n",
        "* `gemma-3-1b-it`\n",
        "  * Supports only text input and English language\n",
        "  * 32k context length\n",
        "\n",
        "* `gemma-3-4b-it`, `gemma-3-12b-it`, `gemma-3-27b-it`\n",
        "  * Supports both text and image input\n",
        "  * Supports 140+ languages\n",
        "  * 128k context length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5bU6AcdHXa3"
      },
      "source": [
        "In the following code block, we are initializing a pretrained Gemma3 model using the HuggingFace library.\n",
        "\n",
        "We first specify the `model_id`.\n",
        "\n",
        "**`Gemma3ForConditionalGeneration`** is a subclass of **`PreTrainedModel`** from the Hugging Face Transformers library, designed for text generation tasks (like chat, summarization, instruction following) using the Gemma 3 model. We will be using the **`from_pretrained()`** method from this class to download and initialize the model.\n",
        "\n",
        "*   **`device_map=\"auto\"`** automatically places the model on the best available device (like a GPU if one is available).\n",
        "*   **`torch_dtype=torch.bfloat16`** sets the precision. `bfloat16` uses less memory and works well on newer GPUs.\n",
        "*   **`.eval()`** puts the model in inference mode (not training), which is important when you're just generating text.\n",
        "\n",
        "The final step is loading the processor, which helps us prepare inputs and decode outputs for the model. It handles:\n",
        "\n",
        "*   Tokenizing text (turning words into numbers the model understands)\n",
        "*   Decoding model outputs back into human-readable text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MpAndEbXjDZ"
      },
      "outputs": [],
      "source": [
        "model_name = 'gemma-3-4b-it' # @param ['gemma-3-1b-it', 'gemma-3-4b-it', 'gemma-3-12b-it', 'gemma-3-27b-it']\n",
        "model_id = f\"google/{model_name}\"\n",
        "\n",
        "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
        ").eval()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5CtOZS-JqB5"
      },
      "source": [
        "## Step 4: Define helper functions\n",
        "To work with images and videos in our vision-language tasks, we define a few utility functions to handle preprocessing and visualization:\n",
        "\n",
        "**`resize_image(image_path)`**\n",
        "\n",
        "Loads an image from the given path and resizes it to a 640x640 box while maintaining the original aspect ratio. This makes sure images are not distorted and fit within the model‚Äôs expected input size.\n",
        "\n",
        "**`get_model_response(img, prompt, model, processor)`**\n",
        "This function sends a prompt + image pair to the model and returns the generated response.\n",
        "It:\n",
        "\n",
        "*   Wraps the image and text in a \"chat-like\" message structure\n",
        "*   Uses the processor to tokenize and format inputs\n",
        "*   Calls the model to generate a response\n",
        "*   Decodes the result into readable text\n",
        "\n",
        "Note: It uses **`torch.inference_mode()`** for efficiency since we're not training.\n",
        "\n",
        "**`extract_frames(video_path, num_frames)`**\n",
        "Extracts a set number of evenly spaced frames from a video along with timestamps.\n",
        "\n",
        "**`show_video(video_path)`**\n",
        "Displays the selected video inline using HTML5, helpful for reviewing which video was processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2cv9-hQqliy"
      },
      "outputs": [],
      "source": [
        "def resize_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    target_width, target_height = 640, 640\n",
        "    # Calculate the target size (maximum width and height).\n",
        "    if target_width and target_height:\n",
        "        max_size = (target_width, target_height)\n",
        "    elif target_width:\n",
        "        max_size = (target_width, img.height)\n",
        "    elif target_height:\n",
        "        max_size = (img.width, target_height)\n",
        "\n",
        "    img.thumbnail(max_size)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_model_response(img: Image, prompt: str, model, processor):\n",
        "    # Prepare the messages for the model.\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant. Reply only with the answer to the question asked, and avoid using additional text in your response like 'here's the answer'.\"}]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Tokenize inputs and prepare for the model.\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, tokenize=True,\n",
        "        return_dict=True, return_tensors=\"pt\"\n",
        "    ).to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    # Generate response from the model.\n",
        "    with torch.inference_mode():\n",
        "        generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "        generation = generation[0][input_len:]\n",
        "\n",
        "    # Decode the response.\n",
        "    response = processor.decode(generation, skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "\n",
        "def extract_frames(video_path, num_frames):\n",
        "    \"\"\"\n",
        "    The function is adapted from:\n",
        "    https://github.com/merveenoyan/smol-vision/blob/main/Gemma_3_for_Video_Understanding.ipynb\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return []\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Calculate the step size to evenly distribute frames across the video.\n",
        "    step = total_frames // num_frames\n",
        "    frames = []\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        frame_idx = i * step\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        timestamp = round(frame_idx / fps, 2)\n",
        "        frames.append((img, timestamp))\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def show_video(video_path, video_width = 600):\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  video_html = f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\"\n",
        "  return HTML(video_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq76W9ihIR9V"
      },
      "source": [
        "## Step 5: Inference on images\n",
        "We will fetch and download some sample images from github for inferencing. Feel free to upload your own in the temporary Colab storage ('/content/') -> üìÅ icon in left sidebar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVFbiSlvI6C8"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/tahreemrasul/gemma3_inference/main/assets/image_1.png -O /content/image_1.png\n",
        "!wget https://raw.githubusercontent.com/tahreemrasul/gemma3_inference/main/assets/image_2.png -O /content/image_2.png\n",
        "!wget https://raw.githubusercontent.com/tahreemrasul/gemma3_inference/main/assets/image_3.png -O /content/image_3.png\n",
        "!wget https://raw.githubusercontent.com/tahreemrasul/gemma3_inference/main/assets/image_4.png -O /content/image_4.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnXumaSoo9SZ"
      },
      "source": [
        "#### Example 1: Describe an image\n",
        "\n",
        "In the first experiment, we will simply pass the image alongwith a prompt and ask the model to describe what it sees in the image. Feel free to play around with this and see what you get back from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYl5s_SRhN8w"
      },
      "outputs": [],
      "source": [
        "image_file = 'image_1.png' # @param {type: 'string'}\n",
        "prompt = \"Describe the image.\" # @param {type: 'string'}\n",
        "\n",
        "\n",
        "img = resize_image(image_file)\n",
        "display(img)\n",
        "response = get_model_response(img, prompt, model, processor)\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-EqhTGypGra"
      },
      "source": [
        "#### Example 2: Identify landmark\n",
        "\n",
        "In the second experiment, we are going to be a bit more detailed. We will pass it the image alongwith a prompt to identify the landmark in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrCzrjmNlQ45"
      },
      "outputs": [],
      "source": [
        "image_file = 'image_2.png' # @param {type: 'string'}\n",
        "prompt = \"Identify the famous landmark and location.\" # @param {type: 'string'}\n",
        "\n",
        "\n",
        "img = resize_image(image_file)\n",
        "display(img)\n",
        "response = get_model_response(img, prompt, model, processor)\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Npm2HSs2Yo"
      },
      "source": [
        "#### Example 3: Multilingual prompt\n",
        "\n",
        "> The prompt is in Roman Urdu language which translates to: \"What's present in this image?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B053c2aPlQ67"
      },
      "outputs": [],
      "source": [
        "image_file = 'image_3.png' # @param {type: 'string'}\n",
        "prompt = \" ÿßÿ≥ ÿ™ÿµŸà€åÿ± ŸÖ€å⁄∫  ⁄©€åÿß €Å€í\" # @param {type: 'string'}\n",
        "\n",
        "\n",
        "img = resize_image(image_file)\n",
        "display(img)\n",
        "response = get_model_response(img, prompt, model, processor)\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHkrpHDKmmrF"
      },
      "source": [
        "> As you can see, it gave the response in the same language (Urdu here), in which it was prompted. ÿß€å⁄© ⁄Ø€åŸÑÿ±€å ŸÖ€å⁄∫ ŸÅŸÜ Ÿæÿßÿ±€Å which translates to piece of art in a gallery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83-qUPV6sTn"
      },
      "source": [
        "#### Example 4: Mathematical Reasoning\n",
        "\n",
        "Our fourth experiment will be around sharing an image containing a mathematical equation. We will pass this image alongwith a prompt, and ask the model to find the value of the unknown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwAd0UcV6va3"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import Markdown\n",
        "\n",
        "image_file = 'image_4.png' # @param {type: 'string'}\n",
        "prompt = \"What is the value of x?\" # @param {type: 'string'}\n",
        "\n",
        "\n",
        "img = resize_image(image_file)\n",
        "display(img)\n",
        "response = get_model_response(img, prompt, model, processor)\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CF03VDnnAHl"
      },
      "source": [
        "## Step 6: Inference on videos\n",
        "\n",
        "Download a sample video.\n",
        "\n",
        "The video is a clip from the movie \"The Godfather\".\n",
        "* Credits: Paramount Movies\n",
        "* Source: [YouTube](https://www.youtube.com/watch?v=eZHsmb4ezEk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F60ywwom2TYf"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/tahreemrasul/gemma3_inference/main/assets/video.mp4 -O /content/video.mp4\n",
        "!mkdir -p /content/frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4Xk6X7V2nc-"
      },
      "outputs": [],
      "source": [
        "# Video file.\n",
        "video_path = \"video.mp4\" # @param {type : 'string'}\n",
        "\n",
        "# No. of frames to be extracted from the video.\n",
        "num_frames = 2 # @param {type : 'integer'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E72dvnRUED0n",
        "outputId": "10554b1a-a7ae-4bc3-9ba0-3d4f4d7c2a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ],
      "source": [
        "video_output = show_video(video_path, video_width=800)\n",
        "display(video_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbJvZx41ahg5"
      },
      "source": [
        "In this step, we:\n",
        "\n",
        "*   Extract a few key frames from the video using our extract_frames function\n",
        "*   Construct a multi-turn chat prompt that includes both text and images for the model\n",
        "\n",
        "For each extracted frame:\n",
        "\n",
        "*   We add a timestamp so the model knows when the frame appears\n",
        "*   We save the frame locally and add it to the prompt as an image reference\n",
        "\n",
        "\n",
        "üí° This creates a multi-modal prompt that tells the model:\n",
        "> Here's a series of frames from a video, please summarize what's happening.‚Äù\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n4VcDXnZ7uE"
      },
      "outputs": [],
      "source": [
        "video_frames = extract_frames(video_path, num_frames=num_frames)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": \"Please summarize what is happening in this video.\"}]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "# Add frames to the messages structure.\n",
        "for frame_data in video_frames:\n",
        "    img, timestamp = frame_data\n",
        "    messages[1][\"content\"].append({\"type\": \"text\", \"text\": f\"Frame at {timestamp} seconds:\"})\n",
        "    img.save(f\"/content/frames/frame_{timestamp}.png\")\n",
        "    messages[1][\"content\"].append({\"type\": \"image\", \"url\": f\"/content/frames/frame_{timestamp}.png\"})\n",
        "\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTI2nBV4bRPV"
      },
      "source": [
        "Now that we've prepared our prompt with images and text, we pass it to the model for generation.\n",
        "\n",
        "Here's what we are doing:\n",
        "\n",
        "*   **`apply_chat_template(...)`** formats and tokenizes the messages into a format the model understands.\n",
        "*   We move the input to the correct device (like a GPU) using **`.to(model.device)`**.\n",
        "*   We generate a response using the model with** `.generate(...)`**. We set **`max_new_tokens=500`** to limit the length of the output and **`do_sample=False`** to make it deterministic (same input = same output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OVynxk_c8Jn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDUZIsaheJX2"
      },
      "source": [
        "You might run into OutOfMemory Cuda issues by executing the cell below. For this reason, we have cleared cache above. You can decrease the number of frames and see if that works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS2MbOTi1Jde"
      },
      "outputs": [],
      "source": [
        "inputs = processor.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, tokenize=True,\n",
        "    return_dict=True, return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "\n",
        "input_length = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "# Generate a response based on the inputs.\n",
        "output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "output = output[0][input_length:]\n",
        "response = processor.decode(output, skip_special_tokens=True)\n",
        "\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvAMHqiP6Bfn"
      },
      "source": [
        "Congratulations on successfully completing this tutorial."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}